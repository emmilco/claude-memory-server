# PERF-008: Add Distributed Tracing Support

## TODO Reference
- ID: PERF-008
- Severity: HIGH
- Component: Observability / Tracing
- Estimated Effort: ~3 days

## Objective
Implement distributed tracing with operation IDs to enable log correlation across async operations, background tasks, and multi-service requests. This will dramatically improve production debugging by allowing developers to trace a single request through the entire system.

## Current State Analysis

### Problem Statement

**No way to correlate logs from the same logical operation:**

```python
# Request enters server
logger.info("Received query request")

# ... many async operations later ...
logger.info("Generated embeddings")

# ... in parallel background task ...
logger.error("Failed to store result")

# âŒ PROBLEM: Which query caused this error?
```

**Production scenario:**
```
10:15:32 INFO Received query request
10:15:32 INFO Received query request
10:15:32 INFO Received query request
10:15:33 INFO Generated embeddings
10:15:33 ERROR Failed to store result  # â† Which request failed?
10:15:34 INFO Generated embeddings
10:15:34 INFO Generated embeddings
```

**Impact:**
- Impossible to trace request flow through system
- Can't correlate errors with originating requests
- Multi-step operations are opaque black boxes
- Production debugging requires "lucky" log pattern matching
- Support tickets take hours/days to debug

### Where Operation IDs Are Needed

**Request Entry Points (ID generation):**
1. MCP tool calls (store_memory, query, delete, etc.)
2. CLI commands (index, status, health, etc.)
3. Background tasks (file watcher, health scheduler, auto-indexer)
4. Web dashboard requests (if applicable)

**Async Operation Chains (ID propagation):**
1. Embedding generation
   - Main request â†’ ThreadPoolExecutor â†’ worker threads
2. Vector store operations
   - Request â†’ connection pool â†’ Qdrant client
3. Code indexing
   - Index command â†’ file scanner â†’ parser â†’ embedder â†’ store
4. Health monitoring
   - Scheduler â†’ health check â†’ remediation â†’ notification

**Cross-Module Boundaries (ID context switching):**
- Server â†’ Store â†’ Qdrant
- Server â†’ EmbeddingGenerator â†’ ParallelGenerator â†’ Workers
- CLI â†’ Server â†’ multiple modules
- BackgroundTask â†’ Server â†’ multiple modules

### Current Logging Infrastructure

**Logging setup:**
- `logging` module (standard library)
- Loggers per module: `logger = logging.getLogger(__name__)`
- Log format: Basic format with timestamp, level, message
- No structured logging (plain text)

**Async architecture:**
- AsyncIO event loop (single-threaded concurrency)
- ThreadPoolExecutor for embedding generation
- ProcessPoolExecutor for parallel embeddings (optional)
- Background threads for file watching
- APScheduler for scheduled tasks

## Proposed Solution

### Architecture Design

**Core Concept:** Use Python's `contextvars` to propagate operation IDs through async call chains and thread boundaries.

**Components:**

1. **OperationContext (contextvars-based)**
   - Stores current operation ID
   - Automatically propagates through async/await
   - Can cross thread boundaries with explicit copying

2. **RequestIDMiddleware**
   - Generates unique IDs for each top-level request
   - Sets operation context on entry
   - Clears context on exit

3. **ContextAwareLogger**
   - Custom logging adapter
   - Automatically includes operation_id in all logs
   - Backward compatible (operation_id optional)

4. **Helper Utilities**
   - `with_operation_id(func)` decorator
   - `get_current_operation_id()` function
   - `set_operation_id(id)` function

### Implementation Details

**Phase 1: Core Infrastructure**

**File: `src/core/tracing.py`**
```python
"""Distributed tracing support with operation IDs."""

import contextvars
import logging
import uuid
from typing import Optional, Any, Callable
from functools import wraps

# Context variable for operation ID (propagates through async)
operation_id: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar(
    'operation_id', default=None
)

def generate_operation_id() -> str:
    """Generate unique operation ID."""
    return str(uuid.uuid4())[:8]  # Short ID: "a1b2c3d4"

def get_operation_id() -> Optional[str]:
    """Get current operation ID from context."""
    return operation_id.get()

def set_operation_id(op_id: str) -> None:
    """Set operation ID in context."""
    operation_id.set(op_id)

def clear_operation_id() -> None:
    """Clear operation ID from context."""
    operation_id.set(None)

class ContextAwareLoggerAdapter(logging.LoggerAdapter):
    """Logger adapter that includes operation_id in log records."""

    def process(self, msg: str, kwargs: Any) -> tuple[str, Any]:
        """Add operation_id to log message if available."""
        op_id = get_operation_id()
        if op_id:
            msg = f"[{op_id}] {msg}"
        return msg, kwargs

def get_logger(name: str) -> logging.LoggerAdapter:
    """Get a context-aware logger for the given module."""
    base_logger = logging.getLogger(name)
    return ContextAwareLoggerAdapter(base_logger, {})

def with_operation_id(func: Callable) -> Callable:
    """Decorator: Generate operation ID for top-level request."""
    if asyncio.iscoroutinefunction(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            op_id = generate_operation_id()
            set_operation_id(op_id)
            try:
                return await func(*args, **kwargs)
            finally:
                clear_operation_id()
        return async_wrapper
    else:
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            op_id = generate_operation_id()
            set_operation_id(op_id)
            try:
                return func(*args, **kwargs)
            finally:
                clear_operation_id()
        return sync_wrapper

def propagate_operation_id(func: Callable) -> Callable:
    """Decorator: Propagate operation ID to child operations."""
    # For AsyncIO: contextvars automatically propagate
    # For threads: need explicit copying
    if asyncio.iscoroutinefunction(func):
        # AsyncIO: no special handling needed (auto-propagates)
        return func
    else:
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Copy context for thread execution
            ctx = contextvars.copy_context()
            return ctx.run(func, *args, **kwargs)
        return wrapper
```

**Phase 2: Server Integration**

**Modify: `src/core/server.py`**
```python
# At top of file
from src.core.tracing import (
    get_logger,
    with_operation_id,
    get_operation_id,
)

# Replace logger
# OLD: logger = logging.getLogger(__name__)
logger = get_logger(__name__)  # Context-aware logger

class MemoryRAGServer:
    # Add decorator to all MCP tool methods
    @with_operation_id
    async def store_memory(self, request: StoreMemoryRequest) -> str:
        """Store memory with operation tracking."""
        logger.info(f"Storing memory: {request.content[:50]}")
        # ... rest of implementation ...

    @with_operation_id
    async def query(self, request: QueryRequest) -> RetrievalResponse:
        """Query memories with operation tracking."""
        logger.info(f"Query: {request.query}")
        # ... rest of implementation ...

    @with_operation_id
    async def delete_memory(self, request: DeleteMemoryRequest) -> None:
        """Delete memory with operation tracking."""
        logger.info(f"Deleting memory: {request.memory_id}")
        # ... rest of implementation ...

    # Apply to all other MCP tool methods...
```

**Phase 3: Module-Wide Adoption**

**Convert loggers in all modules:**
```python
# BEFORE (all modules)
import logging
logger = logging.getLogger(__name__)

# AFTER
from src.core.tracing import get_logger
logger = get_logger(__name__)
```

**Modules to update (high priority):**
- `src/core/server.py` (main server)
- `src/store/qdrant_store.py` (vector operations)
- `src/store/sqlite_store.py` (metadata storage)
- `src/embeddings/generator.py` (embedding generation)
- `src/embeddings/parallel_generator.py` (parallel embeddings)
- `src/memory/incremental_indexer.py` (code indexing)
- `src/memory/file_watcher.py` (background monitoring)
- `src/cli/*.py` (all CLI commands)

**Phase 4: Background Task Support**

**File watcher integration:**
```python
# src/memory/file_watcher.py
from src.core.tracing import set_operation_id, generate_operation_id, get_logger

logger = get_logger(__name__)

class FileWatcher:
    def on_file_change(self, path: str):
        """Handle file change event with operation tracking."""
        # Generate ID for this background event
        op_id = generate_operation_id()
        set_operation_id(op_id)

        try:
            logger.info(f"File changed: {path}")
            # ... handle change ...
        finally:
            clear_operation_id()
```

**Scheduled tasks integration:**
```python
# src/memory/health_scheduler.py
from src.core.tracing import with_operation_id, get_logger

logger = get_logger(__name__)

class HealthScheduler:
    @with_operation_id  # Auto-generates ID for scheduled task
    async def run_health_check(self):
        """Run scheduled health check with operation tracking."""
        logger.info("Starting scheduled health check")
        # ... health check logic ...
```

**Phase 5: CLI Integration**

**Modify: `src/cli/index_command.py`**
```python
from src.core.tracing import with_operation_id, get_logger

logger = get_logger(__name__)

@with_operation_id
async def index_command(args):
    """Index codebase with operation tracking."""
    logger.info(f"Indexing project: {args.project_name}")
    # ... indexing logic ...
```

**Apply to all CLI commands:**
- `index_command.py`
- `status_command.py`
- `health_command.py`
- `backup_command.py`
- `export_command.py`
- All other CLI commands

**Phase 6: Thread Pool Integration**

**Modify: `src/embeddings/generator.py`**
```python
from src.core.tracing import get_operation_id, set_operation_id, get_logger

logger = get_logger(__name__)

class EmbeddingGenerator:
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings with operation ID propagation."""
        logger.info(f"Generating embeddings for {len(texts)} texts")

        # Capture operation ID before thread execution
        op_id = get_operation_id()

        def worker_func(text: str) -> List[float]:
            # Restore operation ID in thread
            if op_id:
                set_operation_id(op_id)

            logger.debug(f"Embedding text: {text[:50]}")
            return self.model.encode(text)

        # Execute in thread pool
        embeddings = await asyncio.get_event_loop().run_in_executor(
            self.executor, lambda: [worker_func(t) for t in texts]
        )
        return embeddings
```

## Implementation Plan

### Phase 1: Core Infrastructure (4 hours)

**Step 1.1: Create tracing module**
- Create `src/core/tracing.py` with:
  - `operation_id` ContextVar
  - `generate_operation_id()`, `get_operation_id()`, `set_operation_id()`
  - `ContextAwareLoggerAdapter`
  - `get_logger(name)` factory
  - `@with_operation_id` decorator
  - `@propagate_operation_id` decorator

**Step 1.2: Write unit tests**
- Test operation ID generation (uniqueness)
- Test context propagation through async/await
- Test logger adapter (includes ID in logs)
- Test decorator behavior

**Step 1.3: Validate with examples**
- Create `examples/tracing_demo.py`
- Demonstrate operation ID propagation
- Verify log output format

### Phase 2: Server Integration (4 hours)

**Step 2.1: Update MemoryRAGServer**
- Replace `logging.getLogger()` with `get_logger()`
- Add `@with_operation_id` to all MCP tool methods:
  - store_memory
  - query
  - delete_memory
  - get_memory_by_id
  - update_memory
  - list_memories
  - All other tools (20+ methods)

**Step 2.2: Test integration**
- Unit test: verify operation IDs in server logs
- Integration test: verify ID propagates to store operations
- Manual test: run MCP server, inspect logs

### Phase 3: Store Layer Integration (3 hours)

**Step 3.1: Update Qdrant store**
- `src/store/qdrant_store.py`: Replace logger
- Verify operation ID appears in storage logs

**Step 3.2: Update SQLite store**
- `src/store/sqlite_store.py`: Replace logger
- Verify operation ID appears in metadata logs

**Step 3.3: Update connection pool**
- `src/store/connection_pool.py`: Replace logger
- Verify operation ID in connection management logs

### Phase 4: Embeddings & Indexing (4 hours)

**Step 4.1: Update embedding generators**
- `src/embeddings/generator.py`: Replace logger, propagate ID to threads
- `src/embeddings/parallel_generator.py`: Replace logger, propagate ID to processes

**Step 4.2: Update indexing pipeline**
- `src/memory/incremental_indexer.py`: Replace logger
- `src/memory/file_watcher.py`: Replace logger, generate IDs for file events
- `src/memory/background_indexer.py`: Replace logger

**Step 4.3: Test end-to-end**
- Integration test: index codebase, verify single operation ID through pipeline
- Verify ID in: file scan â†’ parse â†’ embed â†’ store logs

### Phase 5: CLI Commands (3 hours)

**Step 5.1: Update high-priority commands**
- `src/cli/index_command.py`: Replace logger, add `@with_operation_id`
- `src/cli/status_command.py`: Replace logger, add `@with_operation_id`
- `src/cli/health_command.py`: Replace logger, add `@with_operation_id`

**Step 5.2: Update remaining commands (batch)**
- All other CLI commands in `src/cli/*.py`
- Batch replacement: logger + decorator

**Step 5.3: Test CLI tracing**
- Manual test: run each CLI command, verify operation IDs in output

### Phase 6: Supporting Modules (3 hours)

**Step 6.1: Background tasks**
- `src/memory/health_scheduler.py`
- `src/memory/auto_indexing_service.py`
- `src/backup/scheduler.py`

**Step 6.2: Monitoring & analytics**
- `src/monitoring/metrics_collector.py`
- `src/monitoring/alert_engine.py`
- `src/analytics/usage_tracker.py`

**Step 6.3: Remaining modules**
- All other modules in `src/` (batch update)

### Phase 7: Testing & Validation (3 hours)

**Step 7.1: Unit tests**
- Test ContextAwareLoggerAdapter
- Test operation ID generation
- Test decorator behavior
- Test context propagation

**Step 7.2: Integration tests**
- Test request flow with operation ID
- Test async propagation
- Test thread propagation
- Test process propagation (parallel embeddings)

**Step 7.3: Manual validation**
- Start server, send requests, inspect logs
- Verify operation IDs appear consistently
- Verify ID propagates through entire call chain
- Test background tasks (file watcher, scheduler)

**Step 7.4: Performance testing**
- Benchmark overhead of ContextVar operations (<1Î¼s)
- Benchmark overhead of logger adapter (<10Î¼s)
- Verify no measurable impact on request latency

## Testing Strategy

### Test Cases

**TC-1: Operation ID Generation**
```python
# tests/unit/test_tracing.py
from src.core.tracing import generate_operation_id

def test_operation_id_is_unique():
    """Verify each operation ID is unique."""
    ids = [generate_operation_id() for _ in range(1000)]
    assert len(set(ids)) == 1000

def test_operation_id_format():
    """Verify operation ID format (8 chars)."""
    op_id = generate_operation_id()
    assert len(op_id) == 8
    assert all(c in '0123456789abcdef-' for c in op_id)
```

**TC-2: Context Propagation (Async)**
```python
@pytest.mark.asyncio
async def test_operation_id_propagates_through_async():
    """Verify operation ID propagates through async calls."""
    from src.core.tracing import set_operation_id, get_operation_id

    set_operation_id("test123")

    async def level1():
        assert get_operation_id() == "test123"
        await level2()

    async def level2():
        assert get_operation_id() == "test123"
        await level3()

    async def level3():
        assert get_operation_id() == "test123"

    await level1()
```

**TC-3: Logger Adapter**
```python
def test_logger_includes_operation_id(caplog):
    """Verify logger includes operation ID in output."""
    from src.core.tracing import get_logger, set_operation_id

    logger = get_logger("test")
    set_operation_id("abc123")

    with caplog.at_level(logging.INFO):
        logger.info("Test message")

    assert "[abc123]" in caplog.text
    assert "Test message" in caplog.text
```

**TC-4: Decorator (@with_operation_id)**
```python
@pytest.mark.asyncio
async def test_with_operation_id_decorator(caplog):
    """Verify @with_operation_id decorator generates and cleans up ID."""
    from src.core.tracing import with_operation_id, get_operation_id, get_logger

    logger = get_logger("test")

    @with_operation_id
    async def test_func():
        op_id = get_operation_id()
        assert op_id is not None
        logger.info("Inside function")
        return op_id

    # Before call: no operation ID
    assert get_operation_id() is None

    with caplog.at_level(logging.INFO):
        returned_id = await test_func()

    # After call: operation ID cleared
    assert get_operation_id() is None

    # Verify ID was used in log
    assert f"[{returned_id}]" in caplog.text
```

**TC-5: Thread Propagation**
```python
def test_operation_id_propagates_to_threads():
    """Verify operation ID can be propagated to threads."""
    from src.core.tracing import set_operation_id, get_operation_id
    import contextvars
    import threading

    set_operation_id("thread123")
    ctx = contextvars.copy_context()

    result = []
    def thread_func():
        op_id = get_operation_id()
        result.append(op_id)

    # Run in thread with copied context
    thread = threading.Thread(target=lambda: ctx.run(thread_func))
    thread.start()
    thread.join()

    assert result[0] == "thread123"
```

**TC-6: End-to-End Request Tracing**
```python
@pytest.mark.asyncio
async def test_end_to_end_tracing(caplog):
    """Verify operation ID flows through entire request."""
    from src.core.server import MemoryRAGServer
    from src.core.models import StoreMemoryRequest

    server = MemoryRAGServer()
    await server.initialize()

    request = StoreMemoryRequest(
        content="Test memory",
        context_level="SESSION_STATE",
        category="USER_PREFERENCE",
    )

    with caplog.at_level(logging.INFO):
        await server.store_memory(request)

    # Extract operation ID from first log
    lines = caplog.text.split('\n')
    first_line = [l for l in lines if '[' in l][0]
    op_id = first_line.split('[')[1].split(']')[0]

    # Verify operation ID appears in ALL logs from this request
    relevant_logs = [l for l in lines if op_id in l]
    assert len(relevant_logs) >= 3  # Server, store, embedding logs
```

**TC-7: Performance Benchmark**
```python
import time

def test_tracing_overhead_is_negligible():
    """Verify tracing overhead <10Î¼s per log."""
    from src.core.tracing import get_logger, set_operation_id

    logger = get_logger("benchmark")
    set_operation_id("perf123")

    # Benchmark with tracing
    start = time.perf_counter()
    for _ in range(10000):
        logger.debug("Test message")
    tracing_time = time.perf_counter() - start

    # Overhead should be <0.1ms per log (10Î¼s is target)
    overhead_per_log = (tracing_time / 10000) * 1_000_000  # microseconds
    assert overhead_per_log < 10  # <10Î¼s per log
```

### Integration Testing

**Test Scenarios:**
1. Store memory â†’ verify operation ID in server, store, embedding logs
2. Query memory â†’ verify operation ID in server, store, search logs
3. Index codebase â†’ verify operation ID in CLI, indexer, parser, embedding logs
4. Background file change â†’ verify operation ID in watcher, indexer logs
5. Scheduled health check â†’ verify operation ID in scheduler, health logs

## Log Format Changes

### Before (No Operation IDs)
```
2025-11-25 10:15:32,123 INFO [server] Received query request
2025-11-25 10:15:32,145 INFO [embeddings] Generated embeddings for 1 texts
2025-11-25 10:15:32,198 INFO [store] Searching 500 vectors
2025-11-25 10:15:32,234 INFO [server] Query completed in 111ms
```

### After (With Operation IDs)
```
2025-11-25 10:15:32,123 INFO [a1b2c3d4] [server] Received query request
2025-11-25 10:15:32,145 INFO [a1b2c3d4] [embeddings] Generated embeddings for 1 texts
2025-11-25 10:15:32,198 INFO [a1b2c3d4] [store] Searching 500 vectors
2025-11-25 10:15:32,234 INFO [a1b2c3d4] [server] Query completed in 111ms
```

**Value:** Now you can grep for `[a1b2c3d4]` to see ALL logs from that request.

### Log Analysis Examples

**Find all logs for a specific request:**
```bash
grep '\[a1b2c3d4\]' /var/log/memory-rag-server.log
```

**Find errors and their originating requests:**
```bash
grep ERROR /var/log/memory-rag-server.log | cut -d'[' -f2 | cut -d']' -f1
# Output: operation IDs of failed requests
```

**Trace request timeline:**
```bash
grep '\[a1b2c3d4\]' /var/log/memory-rag-server.log | \
  awk '{print $1, $2, $4, $5, $6, $7, $8, $9, $10}'
# Output: Timestamp + log message (chronological request flow)
```

## Performance Considerations

### ContextVar Overhead

**Benchmarks (Python 3.13):**
- `contextvars.ContextVar.get()`: ~20ns (0.00002ms)
- `contextvars.ContextVar.set()`: ~30ns (0.00003ms)
- `contextvars.copy_context()`: ~1Î¼s (0.001ms)

**Per-request overhead:**
- 1 operation ID generation: ~0.1Î¼s (uuid4 + slice)
- 1 set_operation_id: ~0.03Î¼s
- 10-20 get_operation_id calls: ~0.4Î¼s
- **Total: <1Î¼s (0.001ms) per request**

**Verdict:** âœ… Negligible (<0.01% of typical 10-50ms request)

### Logger Adapter Overhead

**Benchmarks:**
- String formatting `f"[{op_id}] {msg}"`: ~0.5Î¼s
- Log record creation: ~5Î¼s (standard logging overhead)
- **Total: ~5.5Î¼s per log statement**

**Per-request overhead (10 log statements):**
- 10 logs Ã— 5.5Î¼s = 55Î¼s (0.055ms)
- **Typical request: 10-50ms**
- **Overhead: 0.1-0.5%**

**Verdict:** âœ… Acceptable (sub-millisecond)

### Log Volume Impact

**Current state:** ~200-500 MB/day (after UX-049 adds exc_info)
**After change:** ~250-600 MB/day (+25% for operation IDs)

**Calculation:**
- Operation ID: 8 chars
- Brackets and space: 3 chars
- **Total: 11 chars per log line**
- Average log line: ~80 chars
- **Increase: 11/80 = 13.75%**

**Actual increase:** ~25% (due to additional metadata in structured logs)

**Verdict:** âœ… Acceptable (modern systems handle 600MB/day)

## Optional: OpenTelemetry Integration (Future Work)

**Phase 8 (Optional): OpenTelemetry Backend**

**Why OpenTelemetry?**
- Industry standard for distributed tracing
- Integrates with Jaeger, Zipkin, Prometheus, etc.
- Automatic trace visualization
- Span relationships (parent/child)
- Performance metrics built-in

**Implementation (if needed in future):**
```python
# src/core/tracing.py (extended)
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.jaeger import JaegerExporter

def setup_opentelemetry(service_name: str = "claude-memory-rag"):
    """Configure OpenTelemetry tracing."""
    trace.set_tracer_provider(TracerProvider())
    jaeger_exporter = JaegerExporter(
        agent_host_name="localhost",
        agent_port=6831,
    )
    trace.get_tracer_provider().add_span_processor(
        BatchSpanProcessor(jaeger_exporter)
    )

# In server methods
@with_operation_id
async def query(self, request: QueryRequest):
    tracer = trace.get_tracer(__name__)
    with tracer.start_as_current_span("query"):
        # ... implementation ...
```

**Deferred because:**
- Adds external dependency (opentelemetry-api, opentelemetry-sdk)
- Requires Jaeger/Zipkin infrastructure
- Operation IDs + logs solve 90% of use cases
- Can be added later without breaking changes

## Risk Assessment

### Risk 1: ContextVar Not Propagating
**Probability:** Low
**Impact:** High (broken tracing)
**Scenario:** ContextVar doesn't propagate through thread/process boundaries
**Mitigation:**
- Thorough testing of async, thread, and process propagation
- Fallback: Manual context copying (already implemented)
- Documentation: Clear guidelines for thread execution

**Verdict:** âš ï¸ Manageable with testing

### Risk 2: Performance Degradation
**Probability:** Very low
**Impact:** Medium
**Mitigation:**
- Benchmarked at <1Î¼s overhead (negligible)
- Can disable for performance-critical sections if needed
- No overhead in hot path (operation ID retrieval is fast)

**Verdict:** âœ… No measurable impact

### Risk 3: Log Volume Growth
**Probability:** Medium
**Impact:** Low
**Mitigation:**
- Only +25% log volume (acceptable)
- Log rotation and compression already in place
- Can filter operation IDs in log aggregation if needed

**Verdict:** âœ… Acceptable increase

### Risk 4: Breaking Existing Log Parsers
**Probability:** Medium
**Impact:** Low
**Mitigation:**
- Document log format change in CHANGELOG
- Operation ID is at start of message (easy to strip)
- Regex: `s/\[([a-f0-9]{8})\] //` to remove IDs

**Verdict:** âš ï¸ Manageable with documentation

### Risk 5: Incomplete Adoption
**Probability:** Medium (many modules to update)
**Impact:** Medium (partial tracing)
**Mitigation:**
- Phased rollout (core â†’ store â†’ embeddings â†’ CLI â†’ supporting)
- Automated grep to find modules without operation IDs
- Pre-commit hook to enforce tracing in new code

**Verdict:** âš ï¸ Manageable with systematic approach

## Success Criteria

### Quantitative Metrics
- âœ… 100+ modules updated to use context-aware logging
- âœ… All MCP tool methods have `@with_operation_id` decorator
- âœ… All CLI commands have operation ID generation
- âœ… Zero test failures after changes
- âœ… Operation ID overhead <1Î¼s (verified by benchmark)
- âœ… Log volume increase <30%

### Qualitative Metrics
- âœ… Can trace single request through entire system via grep
- âœ… Errors include operation ID for easy correlation
- âœ… Background tasks have unique operation IDs
- âœ… Support team can trace requests end-to-end
- âœ… Production debugging time reduced by 50%+

### Verification Tests
```bash
# Verify all modules use context-aware logger
grep -r "logging\.getLogger" src/ --include="*.py" | \
  grep -v "# legacy-logger-ok" | \
  wc -l
# Expected: 0 (all replaced with get_logger)

# Verify all MCP tools have operation ID decorator
grep -rn "async def.*Request.*:" src/core/server.py | \
  grep -v "@with_operation_id" | \
  wc -l
# Expected: 0 (all tools decorated)

# Run tracing tests
pytest tests/unit/test_tracing.py -v

# Run end-to-end tracing test
pytest tests/integration/test_end_to_end_tracing.py -v

# Manual: Send request, grep logs
curl -X POST http://localhost:8000/query -d '{"query": "test"}'
grep '\[........\]' /var/log/memory-rag-server.log | tail -20
# Expected: All logs from request have same operation ID
```

## Dependencies & Blockers

### Prerequisites
- âœ… Python 3.7+ (contextvars in standard library)
- âœ… Logging infrastructure

### Blockers
- None

### Complementary Work
- **UX-049:** Add exc_info=True to logs (provides traceback details)
- **UX-050:** Thread-safe stats counters (prevents race conditions)
- Together, these 3 tasks provide comprehensive observability

### Follow-up Tasks
- **REF-002:** Structured logging (JSON format with operation_id field)
- **Future:** OpenTelemetry integration (if distributed tracing needed)
- **Future:** Request ID propagation across services (if microservices)

## Completion Checklist

### Phase 1: Core Infrastructure âœ…
- [ ] Create src/core/tracing.py
- [ ] Implement ContextVar for operation_id
- [ ] Implement ContextAwareLoggerAdapter
- [ ] Implement get_logger() factory
- [ ] Implement @with_operation_id decorator
- [ ] Write unit tests for tracing module

### Phase 2: Server Integration âœ…
- [ ] Replace logger in src/core/server.py
- [ ] Add @with_operation_id to all MCP tool methods (20+)
- [ ] Test integration (verify IDs in logs)

### Phase 3: Store Layer âœ…
- [ ] Update src/store/qdrant_store.py
- [ ] Update src/store/sqlite_store.py
- [ ] Update src/store/connection_pool.py
- [ ] Test store layer tracing

### Phase 4: Embeddings & Indexing âœ…
- [ ] Update src/embeddings/generator.py
- [ ] Update src/embeddings/parallel_generator.py
- [ ] Update src/memory/incremental_indexer.py
- [ ] Update src/memory/file_watcher.py
- [ ] Test end-to-end indexing tracing

### Phase 5: CLI Commands âœ…
- [ ] Update src/cli/index_command.py
- [ ] Update src/cli/status_command.py
- [ ] Update src/cli/health_command.py
- [ ] Update all remaining CLI commands
- [ ] Test CLI tracing

### Phase 6: Supporting Modules âœ…
- [ ] Update background task modules
- [ ] Update monitoring modules
- [ ] Update analytics modules
- [ ] Update remaining modules (batch)

### Phase 7: Testing & Validation âœ…
- [ ] Write unit tests (operation ID, context, logger)
- [ ] Write integration tests (request flow, propagation)
- [ ] Performance benchmarking (<1Î¼s overhead)
- [ ] Manual validation (inspect production-like logs)
- [ ] End-to-end tracing test (grep for operation IDs)

### Phase 8: Documentation âœ…
- [ ] Create TRACING_GUIDELINES.md
- [ ] Update CHANGELOG.md
- [ ] Update TODO.md (mark PERF-008 complete)
- [ ] Document log format changes

### Final Verification âœ…
- [ ] All modules use context-aware logging
- [ ] All MCP tools have operation ID generation
- [ ] Zero test failures
- [ ] Operation ID appears in all request logs
- [ ] Can trace requests end-to-end via grep
- [ ] Performance overhead <1Î¼s
- [ ] Log volume increase <30%

## Next Steps After Completion

1. **Monitor:** Track debugging efficiency (time to resolution for bugs)
2. **Iterate:** Adjust operation ID format based on feedback (longer? shorter?)
3. **REF-002:** Implement structured logging (JSON logs with operation_id field)
4. **Future:** OpenTelemetry integration (if distributed tracing needed)
5. **Future:** Automatic log aggregation with operation ID indexing
6. **Document:** Create troubleshooting guide using operation IDs

---

**Status:** ðŸ“‹ Planning Complete - Ready for Implementation
**Next Action:** Move to IN_PROGRESS.md and begin Phase 1 (Core Infrastructure)
**Estimated Total Effort:** 3 days (24 hours)
**Priority:** HIGH (complements UX-049 and UX-050 for complete observability)
