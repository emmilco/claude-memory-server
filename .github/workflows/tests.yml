name: Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      checks: write
      pull-requests: write

    services:
      qdrant:
        image: qdrant/qdrant:latest
        ports:
          - 6333:6333
          - 6334:6334
        env:
          QDRANT_API_KEY: development

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.13'
        cache: 'pip'

    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        toolchain: stable

    - name: Cache Rust compilation
      uses: Swatinem/rust-cache@v2
      with:
        workspaces: rust_core
        cache-on-failure: true

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install maturin
        pip install -r requirements.txt
        pip install pytest pytest-asyncio pytest-cov pytest-timeout

    - name: Build Rust module
      run: |
        cd rust_core
        maturin build --release --interpreter python3.13
        pip install target/wheels/mcp_performance_core-*-cp313-cp313-manylinux*.whl
        cd ..

    - name: Wait for Qdrant
      run: |
        echo "Waiting for Qdrant to be ready..."

        for i in {1..20}; do
          if curl -f http://localhost:6333/ > /dev/null 2>&1; then
            echo "âœ“ Qdrant is ready!"
            curl http://localhost:6333/
            exit 0
          fi
          echo "Attempt $i: Qdrant not ready, waiting..."
          sleep 2
        done

        echo "âŒ Qdrant failed to start"
        exit 1

    - name: Run tests
      env:
        QDRANT_URL: http://localhost:6333
        QDRANT_API_KEY: development
      run: |
        set -o pipefail
        pytest tests/ -v --tb=short --timeout=30 --timeout-method=thread --cov=src --cov-report=term-missing --cov-report=xml --junitxml=test-results.xml 2>&1 | tee pytest-output.txt
        echo "PYTEST_EXIT_CODE=${PIPESTATUS[0]}" >> $GITHUB_ENV

    - name: Generate test summary
      if: always()
      run: |
        echo "## Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract test counts from pytest output
        if [ -f pytest-output.txt ]; then
          SUMMARY_LINE=$(grep -E "^=+ .* (passed|failed|error)" pytest-output.txt | tail -1)
          if [ -n "$SUMMARY_LINE" ]; then
            echo "**Result:** $SUMMARY_LINE" >> $GITHUB_STEP_SUMMARY
          fi

          # Extract coverage percentage
          COV_LINE=$(grep "^TOTAL" pytest-output.txt | tail -1)
          if [ -n "$COV_LINE" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Coverage:**" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "$COV_LINE" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ“Š See [full test logs](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for detailed output" >> $GITHUB_STEP_SUMMARY

    - name: Upload coverage reports
      uses: codecov/codecov-action@v4
      if: always()
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
        token: ${{ secrets.CODECOV_TOKEN }}
      continue-on-error: true

    - name: Publish test results
      uses: EnricoMi/publish-unit-test-result-action@v2
      if: always() && hashFiles('test-results.xml') != ''
      with:
        files: test-results.xml
        check_name: Test Results
        comment_mode: off
